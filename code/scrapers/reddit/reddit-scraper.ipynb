{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1502f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import date\n",
    "import pwlf\n",
    "import datetime\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time, math\n",
    "\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('CllfFuOvEOBS09TAzabT9Q', 'UmOCgu0uvbPzHn6bJgPwZ5Yf6Rmx7w')\n",
    "\n",
    "with open('pw.txt','r') as f:\n",
    "    pw = f.read().rstrip('\\n')\n",
    "# here we pass our login method (password), username, and password\n",
    "login = {'grant_type': 'password',\n",
    "        'username': 'dspd1',\n",
    "        'password': pw}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e474a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db524b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers(login):\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=login, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers['Authorization'] = f'bearer {TOKEN}'\n",
    "\n",
    "    # while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "    requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b06677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_reddits=['r/1P_LSD', 'r/2cb', 'r/4acodmt', 'r/adderall', 'r/afinil', 'r/ambien', 'r/AMT', 'r/Ayahuasca', \n",
    "'r/cocaine', 'r/cripplingalcoholism', 'r/DMT', 'r/DPH', 'r/dxm', 'r/fentanyl', 'r/ketamine', 'r/kratom', \n",
    "'r/LSA', 'r/LSD', 'r/MDMA', 'r/MemantineHCl', 'r/mescaline', 'r/meth', 'r/PCP', \n",
    "'r/phenibut', 'r/PsilocybinMushrooms', 'r/Salvia', 'r/shroomers', 'r/shrooms', \n",
    "'r/benzodiazepines', 'r/dissociatives', 'r/DissonautUniverse', 'r/gabagoodness', \n",
    "'r/microdosing', 'r/noids', 'r/opiates', 'r/Opioid_RCs', 'r/PsychedelicMessages', \n",
    "'r/Psychedelics', 'r/PsychedSubstance', 'r/Psychonaut', 'r/RationalPsychonaut', 'r/Stims', \n",
    "'r/treedibles', 'r/trees', 'r/tryptonaut', 'r/AnAnswerToHeal', 'r/AskDrugNerds', 'r/askdrugs', 'r/aves', \n",
    "'r/bestoferowid', 'r/CurrentlyTripping', 'r/druganalytics', 'r/druggardening', 'r/DrugNerds', 'r/Drugs', \n",
    "'r/DrugShowerThoughts', 'r/DrugsOver30', 'r/erowid', 'r/ObscureDrugs', 'r/ReagentTesting', 'r/researchchemicals', \n",
    "'r/samelevel', 'r/TheDrugClassroom', 'r/TheeHive', 'r/tripreports', 'r/TripSit', 'r/TripTales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02878996",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = get_headers(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f928bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit(subreddit, headers, params, dig_deeper):\n",
    "    res = requests.get(f'https://oauth.reddit.com/{sub_reddit}/new',\n",
    "                              headers=headers,\n",
    "                              params=params)\n",
    "    if 'error' in res.json():\n",
    "        if res.json()['error']==403:\n",
    "            print(f'private subreddit {sub_reddit}')\n",
    "        elif res.json()['error']==404:\n",
    "            reason = res.json()['reason']\n",
    "            print(f'not found {sub_reddit}, reason {reason}')\n",
    "        else:\n",
    "            print(res.json())\n",
    "        \n",
    "        dig_deeper = False\n",
    "    \n",
    "    return res, dig_deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7050d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_from_post(subreddit, post_id):\n",
    "    df_from_comments = pd.DataFrame()\n",
    "\n",
    "    res_comments = requests.get(f'https://oauth.reddit.com/{subreddit}/comments/{post_id}', \n",
    "               params={'depth':3,'sort':'old', 'limit':30}, headers=headers)\n",
    "    try:\n",
    "        if 'error' in res_comments.json():\n",
    "            print(f'error in comments {res_comments.json()}')\n",
    "    except:\n",
    "        print(f' exception while checking error {res_comments}')\n",
    "        \n",
    "    try:\n",
    "        if len(res_comments.json())>1:\n",
    "            df_from_comments = df_comments(res_comments)\n",
    "    except:\n",
    "        print(res_comments)\n",
    "\n",
    "    return df_from_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1e5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response(res):\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        df = pd.concat([df, pd.Series({\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'subreddit_name_prefixed': post['data']['subreddit_name_prefixed'],\n",
    "            'name': post['data']['name'],\n",
    "            'author': post['data']['author'],\n",
    "            'title': post['data']['title'],\n",
    "            'selftext': post['data']['selftext'],\n",
    "            'num_comments':post['data']['num_comments'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'view_count': post['data']['view_count'],\n",
    "            'score': post['data']['score'],\n",
    "            'link_flair_css_class': post['data']['link_flair_css_class'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'id': post['data']['id'],\n",
    "            'kind': post['kind']}).to_frame().T], ignore_index=True)\n",
    "        subreddit = post['data']['subreddit_name_prefixed']\n",
    "        id_ = post['data']['id']\n",
    "#         print(subreddit)\n",
    "#         print(id_)\n",
    "        df_from_comments = get_comments_from_post(subreddit=subreddit, post_id=id_)\n",
    "        df = pd.concat([df, df_from_comments], ignore_index=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7f7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_comments(res):\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()[1]['data']['children']:\n",
    "        if post['kind']=='t1':\n",
    "            df = pd.concat([df, pd.Series({\n",
    "                'subreddit': post['data']['subreddit'],\n",
    "                'subreddit_name_prefixed': post['data']['subreddit_name_prefixed'],\n",
    "                'name': post['data']['name'],\n",
    "                'author': post['data']['author'],\n",
    "                'body': post['data']['body'],\n",
    "                'ups': post['data']['ups'],\n",
    "                'downs': post['data']['downs'],\n",
    "                'score': post['data']['score'],\n",
    "                'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "                'id': post['data']['id'],\n",
    "                'kind': post['kind']}).to_frame().T], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# 'approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', \n",
    "# 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', \n",
    "# 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', \n",
    "# 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', \n",
    "# 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', \n",
    "# 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', \n",
    "# 'is_created_from_ads_ui', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', \n",
    "# 'author_flair_richtext', 'gildings', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type',\n",
    "# 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', \n",
    "# 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', \n",
    "# 'over_18', 'all_awardings', 'awarders', 'media_only', 'can_gild', 'spoiler', 'locked', 'author_flair_text', \n",
    "# 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'author_is_blocked', \n",
    "# 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', \n",
    "# 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', \n",
    "# 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', \n",
    "# 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6fa4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous database, loading from the beginning\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # prev_data = pd.read_csv('reddit.csv')\n",
    "    # prev_data.set_index(['subreddit', 'id'], inplace=True)\n",
    "    # display(prev_data)\n",
    "    # Getting back the objects:\n",
    "    with open('objs.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        prev_data = pickle.load(f)\n",
    "        \n",
    "    load_from_begining = False\n",
    "    print(\"Loaded previous data base - loading delta\")\n",
    "\n",
    "except:\n",
    "    print(f'No previous database, loading from the beginning')\n",
    "    prev_data = pd.DataFrame()\n",
    "    load_from_begining = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537c4dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevous data frame already exists with 172138 entries, continue the broken loop\n",
      "r/1P_LSD\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/2cb\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/4acodmt\n",
      "7\n",
      "empty res, stopped at i=8\n",
      "r/adderall\n",
      "1\n",
      "empty res, stopped at i=2\n",
      "r/afinil\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/ambien\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/AMT\n",
      "0\n",
      "empty res, stopped at i=1\n",
      "r/Ayahuasca\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/cocaine\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/cripplingalcoholism\n",
      "No r/cripplingalcoholism in prev data set, loading from the begining\n",
      "private subreddit r/cripplingalcoholism\n",
      "get subreddit returned dig_deeper false, stopped at i=1\n",
      "r/DMT\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/DPH\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/dxm\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/fentanyl\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/ketamine\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/kratom\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/LSA\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/LSD\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/MDMA\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/MemantineHCl\n",
      "8\n",
      "empty res, stopped at i=9\n",
      "r/mescaline\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/meth\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/PCP\n",
      "3\n",
      "empty res, stopped at i=4\n",
      "r/phenibut\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/PsilocybinMushrooms\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/Salvia\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/shroomers\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/shrooms\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/benzodiazepines\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/dissociatives\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/DissonautUniverse\n",
      "No r/DissonautUniverse in prev data set, loading from the begining\n",
      "not found r/DissonautUniverse, reason banned\n",
      "get subreddit returned dig_deeper false, stopped at i=1\n",
      "r/gabagoodness\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/microdosing\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/noids\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/opiates\n",
      "9\n",
      "empty res, stopped at i=10\n",
      "r/Opioid_RCs\n",
      "No r/Opioid_RCs in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/PsychedelicMessages\n",
      "No r/PsychedelicMessages in prev data set, loading from the begining\n",
      "empty res, stopped at i=4\n",
      "r/Psychedelics\n",
      "No r/Psychedelics in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/PsychedSubstance\n",
      "No r/PsychedSubstance in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/Psychonaut\n",
      "No r/Psychonaut in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/RationalPsychonaut\n",
      "No r/RationalPsychonaut in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/Stims\n",
      "No r/Stims in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/treedibles\n",
      "No r/treedibles in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/trees\n",
      "No r/trees in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/tryptonaut\n",
      "No r/tryptonaut in prev data set, loading from the begining\n",
      "private subreddit r/tryptonaut\n",
      "get subreddit returned dig_deeper false, stopped at i=1\n",
      "r/AnAnswerToHeal\n",
      "No r/AnAnswerToHeal in prev data set, loading from the begining\n",
      "empty res, stopped at i=8\n",
      "r/AskDrugNerds\n",
      "No r/AskDrugNerds in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/askdrugs\n",
      "No r/askdrugs in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/aves\n",
      "No r/aves in prev data set, loading from the begining\n",
      "error in comments {'message': 'Internal Server Error', 'error': 500}\n",
      "<Response [500]>\n",
      " exception while checking error <Response [500]>\n",
      "<Response [500]>\n",
      "empty res, stopped at i=11\n",
      "r/bestoferowid\n",
      "No r/bestoferowid in prev data set, loading from the begining\n",
      "empty res, stopped at i=3\n",
      "r/CurrentlyTripping\n",
      "No r/CurrentlyTripping in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/druganalytics\n",
      "No r/druganalytics in prev data set, loading from the begining\n",
      "empty res, stopped at i=2\n",
      "r/druggardening\n",
      "No r/druggardening in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/DrugNerds\n",
      "No r/DrugNerds in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/Drugs\n",
      "No r/Drugs in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/DrugShowerThoughts\n",
      "No r/DrugShowerThoughts in prev data set, loading from the begining\n",
      "empty res, stopped at i=4\n",
      "r/DrugsOver30\n",
      "No r/DrugsOver30 in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/erowid\n",
      "No r/erowid in prev data set, loading from the begining\n",
      "empty res, stopped at i=3\n",
      "r/ObscureDrugs\n",
      "No r/ObscureDrugs in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/ReagentTesting\n",
      "No r/ReagentTesting in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/researchchemicals\n",
      "No r/researchchemicals in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/samelevel\n",
      "No r/samelevel in prev data set, loading from the begining\n",
      "empty res, stopped at i=4\n",
      "r/TheDrugClassroom\n",
      "No r/TheDrugClassroom in prev data set, loading from the begining\n",
      "empty res, stopped at i=4\n",
      "r/TheeHive\n",
      "No r/TheeHive in prev data set, loading from the begining\n",
      " exception while checking error <Response [500]>\n",
      "<Response [500]>\n",
      "empty res, stopped at i=11\n",
      "r/tripreports\n",
      "No r/tripreports in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/TripSit\n",
      "No r/TripSit in prev data set, loading from the begining\n",
      "empty res, stopped at i=11\n",
      "r/TripTales\n",
      "No r/TripTales in prev data set, loading from the begining\n",
      "empty res, stopped at i=3\n",
      "time to scrap 7916.211212873459\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prev_data_from_broken_loop=data.copy()\n",
    "    prev_data_from_broken_loop.set_index(['subreddit', 'id'], inplace=True)\n",
    "    print(f'prevous data frame already exists with {len(data)} entries, continue the broken loop')\n",
    "    continue_broken_loop = True\n",
    "    \n",
    "except:\n",
    "    data = pd.DataFrame()\n",
    "    print('data initialised as empty')\n",
    "    continue_broken_loop = False\n",
    "\n",
    "start = time.time()\n",
    "for sub_reddit in sub_reddits:\n",
    "    \n",
    "    params = {'limit': 100}\n",
    "    print(sub_reddit)\n",
    "    if load_from_begining==False:# set the last known data entry\n",
    "        try:\n",
    "            params['before'] = prev_data.loc[sub_reddit[2:]].iloc[0]['name']\n",
    "            i=0\n",
    "        except:\n",
    "            print(f'No {sub_reddit} in previous data set, continue to the next one')\n",
    "            continue\n",
    "    elif continue_broken_loop: # loop was broken\n",
    "        try:\n",
    "            params['after'] = prev_data_from_broken_loop.loc[prev_data_from_broken_loop['body'].isna()].loc[sub_reddit[2:]].iloc[-1]['name']\n",
    "            i = len(prev_data_from_broken_loop.loc[prev_data_from_broken_loop['body'].isna()].loc[sub_reddit[2:]])/100\n",
    "            i = math.floor(i)\n",
    "            print(i)\n",
    "        except:\n",
    "            print(f'No {sub_reddit} in prev data set, loading from the begining')\n",
    "            params = {'limit': 100}\n",
    "            i=0\n",
    "    else:\n",
    "        i=0\n",
    "\n",
    "    year_of_comments = datetime.now().year\n",
    "    dig_deeper = True\n",
    "    while dig_deeper:\n",
    "        i += 1\n",
    "        dig_deeper = i<=100 | year_of_comments>2010\n",
    "\n",
    "        res, dig_deeper = get_subreddit(sub_reddit, headers, params, dig_deeper)\n",
    "        if dig_deeper==False:\n",
    "            print(f'get subreddit returned dig_deeper false, stopped at i={i}')\n",
    "            continue\n",
    "        \n",
    "\n",
    "        new_df = df_from_response(res) #create new data frame\n",
    "        if len(new_df)==0:\n",
    "            dig_deeper=False\n",
    "            print(f'empty res, stopped at i={i}')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if load_from_begining:\n",
    "            # take the final row (oldest entry) not the comments\n",
    "            if 'body' in new_df:\n",
    "                row = new_df.loc[new_df['body'].isna()].iloc[-1] \n",
    "            else:\n",
    "                row = new_df.iloc[-1]             \n",
    "            params['after'] = row['name']\n",
    "            a = pd.to_datetime(row['created_utc'])\n",
    "            year_of_comments = a.year\n",
    "\n",
    "        else:\n",
    "            # take the first row (newest entry)\n",
    "            if 'body' in new_df:\n",
    "                row = new_df.loc[new_df['body'].isna()].iloc[0]              \n",
    "            else:\n",
    "                row = new_df.iloc[0]\n",
    "            # add/update fullname in params\n",
    "            params['before'] = row['name']\n",
    "          \n",
    "\n",
    "        # append new_df to data\n",
    "        data = pd.concat([data, new_df], ignore_index=True)\n",
    "end = time.time()\n",
    "print('time to scrap',end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce21b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>view_count</th>\n",
       "      <th>score</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>kind</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1P_LSD</th>\n",
       "      <th>j4v6oth</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j4v6oth</td>\n",
       "      <td>Top_Transition_1095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-18T15:16:37Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>You’d be surprised by how much a singer blotte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j4tur7l</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j4tur7l</td>\n",
       "      <td>oscarcubby10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-18T06:18:35Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>I’ve never heard of anyone boofing or injectin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j4syrw2</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j4syrw2</td>\n",
       "      <td>Sarthak535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-18T02:11:36Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>They are back in the game!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j4st1m5</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j4st1m5</td>\n",
       "      <td>Nitrous_Acidhead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-18T01:31:35Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>Injecting.... LSD???? I'd like to try that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j4srfsv</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j4srfsv</td>\n",
       "      <td>Bruhtatochips23415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-18T01:20:24Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>This could indicate regional selectivity in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">tripreports</th>\n",
       "      <th>ortmte</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t3_ortmte</td>\n",
       "      <td>COVID19_In_My_ANUS</td>\n",
       "      <td>2gs Golden Teachers and 72mg 5-MAPB + hordenin...</td>\n",
       "      <td>I have never written a published a trip report...</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-26T09:12:39Z</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h6k8rlp</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t1_h6k8rlp</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-26T09:12:39Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>\\nRemember to be civil!\\n\\nDon't be afraid to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h6j360b</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t1_h6j360b</td>\n",
       "      <td>AmtheOutsider</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-26T02:00:20Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>Love it! I'm going to be taking 5g of shrooms ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orkq7v</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t3_orkq7v</td>\n",
       "      <td>Zealousideal_Pipe_21</td>\n",
       "      <td>5 grams Liberty Caps…Healing, Vibration and En...</td>\n",
       "      <td>\\nWhat’s happening? Hope everyone is enjoying ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-25T23:44:10Z</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h6inf96</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t1_h6inf96</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-25T23:44:10Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>\\nRemember to be civil!\\n\\nDon't be afraid to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309915 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subreddit_name_prefixed        name                author  \\\n",
       "subreddit   id                                                                  \n",
       "1P_LSD      j4v6oth                r/1P_LSD  t1_j4v6oth   Top_Transition_1095   \n",
       "            j4tur7l                r/1P_LSD  t1_j4tur7l          oscarcubby10   \n",
       "            j4syrw2                r/1P_LSD  t1_j4syrw2            Sarthak535   \n",
       "            j4st1m5                r/1P_LSD  t1_j4st1m5      Nitrous_Acidhead   \n",
       "            j4srfsv                r/1P_LSD  t1_j4srfsv    Bruhtatochips23415   \n",
       "...                                     ...         ...                   ...   \n",
       "tripreports ortmte            r/tripreports   t3_ortmte    COVID19_In_My_ANUS   \n",
       "            h6k8rlp           r/tripreports  t1_h6k8rlp         AutoModerator   \n",
       "            h6j360b           r/tripreports  t1_h6j360b         AmtheOutsider   \n",
       "            orkq7v            r/tripreports   t3_orkq7v  Zealousideal_Pipe_21   \n",
       "            h6inf96           r/tripreports  t1_h6inf96         AutoModerator   \n",
       "\n",
       "                                                                 title  \\\n",
       "subreddit   id                                                           \n",
       "1P_LSD      j4v6oth                                                NaN   \n",
       "            j4tur7l                                                NaN   \n",
       "            j4syrw2                                                NaN   \n",
       "            j4st1m5                                                NaN   \n",
       "            j4srfsv                                                NaN   \n",
       "...                                                                ...   \n",
       "tripreports ortmte   2gs Golden Teachers and 72mg 5-MAPB + hordenin...   \n",
       "            h6k8rlp                                                NaN   \n",
       "            h6j360b                                                NaN   \n",
       "            orkq7v   5 grams Liberty Caps…Healing, Vibration and En...   \n",
       "            h6inf96                                                NaN   \n",
       "\n",
       "                                                              selftext  \\\n",
       "subreddit   id                                                           \n",
       "1P_LSD      j4v6oth                                                NaN   \n",
       "            j4tur7l                                                NaN   \n",
       "            j4syrw2                                                NaN   \n",
       "            j4st1m5                                                NaN   \n",
       "            j4srfsv                                                NaN   \n",
       "...                                                                ...   \n",
       "tripreports ortmte   I have never written a published a trip report...   \n",
       "            h6k8rlp                                                NaN   \n",
       "            h6j360b                                                NaN   \n",
       "            orkq7v   \\nWhat’s happening? Hope everyone is enjoying ...   \n",
       "            h6inf96                                                NaN   \n",
       "\n",
       "                    num_comments upvote_ratio ups downs view_count score  \\\n",
       "subreddit   id                                                             \n",
       "1P_LSD      j4v6oth          NaN          NaN   1     0        NaN     1   \n",
       "            j4tur7l          NaN          NaN   3     0        NaN     3   \n",
       "            j4syrw2          NaN          NaN   1     0        NaN     1   \n",
       "            j4st1m5          NaN          NaN   3     0        NaN     3   \n",
       "            j4srfsv          NaN          NaN   1     0        NaN     1   \n",
       "...                          ...          ...  ..   ...        ...   ...   \n",
       "tripreports ortmte             7          1.0  10     0       None    10   \n",
       "            h6k8rlp          NaN          NaN   1     0        NaN     1   \n",
       "            h6j360b          NaN          NaN   4     0        NaN     4   \n",
       "            orkq7v             4          1.0  16     0       None    16   \n",
       "            h6inf96          NaN          NaN   1     0        NaN     1   \n",
       "\n",
       "                    link_flair_css_class           created_utc kind  \\\n",
       "subreddit   id                                                        \n",
       "1P_LSD      j4v6oth                  NaN  2023-01-18T15:16:37Z   t1   \n",
       "            j4tur7l                  NaN  2023-01-18T06:18:35Z   t1   \n",
       "            j4syrw2                  NaN  2023-01-18T02:11:36Z   t1   \n",
       "            j4st1m5                  NaN  2023-01-18T01:31:35Z   t1   \n",
       "            j4srfsv                  NaN  2023-01-18T01:20:24Z   t1   \n",
       "...                                  ...                   ...  ...   \n",
       "tripreports ortmte                        2021-07-26T09:12:39Z   t3   \n",
       "            h6k8rlp                  NaN  2021-07-26T09:12:39Z   t1   \n",
       "            h6j360b                  NaN  2021-07-26T02:00:20Z   t1   \n",
       "            orkq7v                        2021-07-25T23:44:10Z   t3   \n",
       "            h6inf96                  NaN  2021-07-25T23:44:10Z   t1   \n",
       "\n",
       "                                                                  body  \n",
       "subreddit   id                                                          \n",
       "1P_LSD      j4v6oth  You’d be surprised by how much a singer blotte...  \n",
       "            j4tur7l  I’ve never heard of anyone boofing or injectin...  \n",
       "            j4syrw2                       They are back in the game!!!  \n",
       "            j4st1m5         Injecting.... LSD???? I'd like to try that  \n",
       "            j4srfsv  This could indicate regional selectivity in th...  \n",
       "...                                                                ...  \n",
       "tripreports ortmte                                                 NaN  \n",
       "            h6k8rlp  \\nRemember to be civil!\\n\\nDon't be afraid to ...  \n",
       "            h6j360b  Love it! I'm going to be taking 5g of shrooms ...  \n",
       "            orkq7v                                                 NaN  \n",
       "            h6inf96  \\nRemember to be civil!\\n\\nDon't be afraid to ...  \n",
       "\n",
       "[309915 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if load_from_begining:\n",
    "    data_combined_with_delta = data\n",
    "else:\n",
    "    data_combined_with_delta = pd.concat([data, prev_data.reset_index()], ignore_index=True)\n",
    "    \n",
    "data_combined_with_delta.sort_values(by=['subreddit', 'created_utc'], ascending=(True,False), inplace=True)\n",
    "data_combined_with_delta.set_index(['subreddit', 'id'], inplace=True)\n",
    "\n",
    "display(data_combined_with_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27fffb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_combined_with_delta.to_csv('reddit.csv')  \n",
    "#display(data_combined_with_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20fbdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "data_combined_with_delta['year_month'] = \\\n",
    "     pd.to_datetime(pd.to_datetime(data_combined_with_delta['created_utc']).dt.strftime('%Y-%m'))\n",
    "with open('objs.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(data_combined_with_delta, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3d63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ffe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_combined_with_delta.loc['1P_LSD']['year_month'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca189362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
