{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1502f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import date\n",
    "import pwlf\n",
    "import datetime\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time, math\n",
    "\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('CllfFuOvEOBS09TAzabT9Q', 'UmOCgu0uvbPzHn6bJgPwZ5Yf6Rmx7w')\n",
    "\n",
    "with open('pw.txt','r') as f:\n",
    "    pw = f.read().rstrip('\\n')\n",
    "# here we pass our login method (password), username, and password\n",
    "login = {'grant_type': 'password',\n",
    "        'username': 'dspd1',\n",
    "        'password': pw}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a3854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db524b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers(login):\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=login, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers['Authorization'] = f'bearer {TOKEN}'\n",
    "\n",
    "    # while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "    requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b06677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_reddits=['r/1P_LSD', 'r/2cb', 'r/4acodmt', 'r/adderall', 'r/afinil', 'r/ambien', 'r/AMT', 'r/Ayahuasca', \n",
    "'r/cocaine', 'r/cripplingalcoholism', 'r/DMT', 'r/DPH', 'r/dxm', 'r/fentanyl', 'r/ketamine', 'r/kratom', \n",
    "'r/LSA', 'r/LSD', 'r/MDMA', 'r/MemantineHCl', 'r/mescaline', 'r/meth', 'r/PCP', \n",
    "'r/phenibut', 'r/PsilocybinMushrooms', 'r/Salvia', 'r/shroomers', 'r/shrooms', \n",
    "'r/benzodiazepines', 'r/dissociatives', 'r/DissonautUniverse', 'r/gabagoodness', \n",
    "'r/microdosing', 'r/noids', 'r/opiates', 'r/Opioid_RCs', 'r/PsychedelicMessages', \n",
    "'r/Psychedelics', 'r/PsychedSubstance', 'r/Psychonaut', 'r/RationalPsychonaut', 'r/Stims', \n",
    "'r/treedibles', 'r/trees', 'r/tryptonaut', 'r/AnAnswerToHeal', 'r/AskDrugNerds', 'r/askdrugs', 'r/aves', \n",
    "'r/bestoferowid', 'r/CurrentlyTripping', 'r/druganalytics', 'r/druggardening', 'r/DrugNerds', 'r/Drugs', \n",
    "'r/DrugShowerThoughts', 'r/DrugsOver30', 'r/erowid', 'r/ObscureDrugs', 'r/ReagentTesting', 'r/researchchemicals', \n",
    "'r/samelevel', 'r/TheDrugClassroom', 'r/TheeHive', 'r/tripreports', 'r/TripSit', 'r/TripTales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02878996",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = get_headers(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f928bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit(subreddit, headers, params, dig_deeper):\n",
    "    res = requests.get(f'https://oauth.reddit.com/{sub_reddit}/new',\n",
    "                              headers=headers,\n",
    "                              params=params)\n",
    "    if 'error' in res.json():\n",
    "        if res.json()['error']==403:\n",
    "            print(f'private subreddit {sub_reddit}')\n",
    "        elif res.json()['error']==404:\n",
    "            reason = res.json()['reason']\n",
    "            print(f'not found {sub_reddit}, reason {reason}')\n",
    "        else:\n",
    "            print(res.json())\n",
    "        \n",
    "        dig_deeper = False\n",
    "    \n",
    "    return res, dig_deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7050d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_from_post(subreddit, post_id):\n",
    "    df_from_comments = pd.DataFrame()\n",
    "\n",
    "    res_comments = requests.get(f'https://oauth.reddit.com/{subreddit}/comments/{post_id}', \n",
    "               params={'depth':3,'sort':'old', 'limit':30}, headers=headers)\n",
    "    try:\n",
    "        if 'error' in res_comments.json():\n",
    "            print(f'error in comments {res_comments.json()}')\n",
    "    except:\n",
    "        print(f' exception while checking error {res_comments}')\n",
    "        \n",
    "    try:\n",
    "        if len(res_comments.json())>1:\n",
    "            df_from_comments = df_comments(res_comments)\n",
    "    except:\n",
    "        print(res_comments)\n",
    "\n",
    "    return df_from_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1e5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response(res):\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        df = pd.concat([df, pd.Series({\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'subreddit_name_prefixed': post['data']['subreddit_name_prefixed'],\n",
    "            'name': post['data']['name'],\n",
    "            'author': post['data']['author'],\n",
    "            'title': post['data']['title'],\n",
    "            'selftext': post['data']['selftext'],\n",
    "            'num_comments':post['data']['num_comments'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'view_count': post['data']['view_count'],\n",
    "            'score': post['data']['score'],\n",
    "            'link_flair_css_class': post['data']['link_flair_css_class'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'id': post['data']['id'],\n",
    "            'kind': post['kind']}).to_frame().T], ignore_index=True)\n",
    "        subreddit = post['data']['subreddit_name_prefixed']\n",
    "        id_ = post['data']['id']\n",
    "#         print(subreddit)\n",
    "#         print(id_)\n",
    "        df_from_comments = get_comments_from_post(subreddit=subreddit, post_id=id_)\n",
    "        df = pd.concat([df, df_from_comments], ignore_index=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7f7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_comments(res):\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()[1]['data']['children']:\n",
    "        if post['kind']=='t1':\n",
    "            df = pd.concat([df, pd.Series({\n",
    "                'subreddit': post['data']['subreddit'],\n",
    "                'subreddit_name_prefixed': post['data']['subreddit_name_prefixed'],\n",
    "                'name': post['data']['name'],\n",
    "                'author': post['data']['author'],\n",
    "                'body': post['data']['body'],\n",
    "                'ups': post['data']['ups'],\n",
    "                'downs': post['data']['downs'],\n",
    "                'score': post['data']['score'],\n",
    "                'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "                'id': post['data']['id'],\n",
    "                'kind': post['kind']}).to_frame().T], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# 'approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', \n",
    "# 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', \n",
    "# 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', \n",
    "# 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', \n",
    "# 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', \n",
    "# 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', \n",
    "# 'is_created_from_ads_ui', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', \n",
    "# 'author_flair_richtext', 'gildings', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type',\n",
    "# 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', \n",
    "# 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', \n",
    "# 'over_18', 'all_awardings', 'awarders', 'media_only', 'can_gild', 'spoiler', 'locked', 'author_flair_text', \n",
    "# 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'author_is_blocked', \n",
    "# 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', \n",
    "# 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', \n",
    "# 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', \n",
    "# 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6fa4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous database, loading from the beginning\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # prev_data = pd.read_csv('reddit.csv')\n",
    "    # prev_data.set_index(['subreddit', 'id'], inplace=True)\n",
    "    # display(prev_data)\n",
    "    # Getting back the objects:\n",
    "    with open('objs.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        prev_data = pickle.load(f)\n",
    "        \n",
    "    load_from_begining = False\n",
    "    print(\"Loaded previous data base - loading delta\")\n",
    "\n",
    "except:\n",
    "    print(f'No previous database, loading from the beginning')\n",
    "    prev_data = pd.DataFrame()\n",
    "    load_from_begining = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537c4dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data initialised as empty\n",
      "r/1P_LSD\n",
      "empty res, stopped at i=11\n",
      "r/2cb\n",
      "empty res, stopped at i=11\n",
      "r/4acodmt\n",
      "empty res, stopped at i=9\n",
      "r/adderall\n",
      "empty res, stopped at i=3\n",
      "r/afinil\n",
      "empty res, stopped at i=11\n",
      "r/ambien\n",
      "empty res, stopped at i=11\n",
      "r/AMT\n",
      "empty res, stopped at i=2\n",
      "r/Ayahuasca\n",
      "empty res, stopped at i=11\n",
      "r/cocaine\n",
      "empty res, stopped at i=11\n",
      "r/cripplingalcoholism\n",
      "private subreddit r/cripplingalcoholism\n",
      "get subreddit returned dig_deeper false, stopped at i=1\n",
      "r/DMT\n",
      " exception while checking error <Response [504]>\n",
      "<Response [504]>\n",
      "empty res, stopped at i=11\n",
      "r/DPH\n",
      "empty res, stopped at i=11\n",
      "r/dxm\n",
      "empty res, stopped at i=11\n",
      "r/fentanyl\n",
      "empty res, stopped at i=11\n",
      "r/ketamine\n",
      "empty res, stopped at i=11\n",
      "r/kratom\n",
      "empty res, stopped at i=11\n",
      "r/LSA\n",
      "empty res, stopped at i=11\n",
      "r/LSD\n",
      "empty res, stopped at i=11\n",
      "r/MDMA\n",
      "empty res, stopped at i=11\n",
      "r/MemantineHCl\n",
      "empty res, stopped at i=10\n",
      "r/mescaline\n",
      "empty res, stopped at i=11\n",
      "r/meth\n",
      "empty res, stopped at i=11\n",
      "r/PCP\n",
      "empty res, stopped at i=5\n",
      "r/phenibut\n",
      "empty res, stopped at i=11\n",
      "r/PsilocybinMushrooms\n",
      "empty res, stopped at i=11\n",
      "r/Salvia\n",
      "empty res, stopped at i=11\n",
      "r/shroomers\n",
      "empty res, stopped at i=11\n",
      "r/shrooms\n",
      "empty res, stopped at i=11\n",
      "r/benzodiazepines\n",
      "empty res, stopped at i=11\n",
      "r/dissociatives\n",
      "empty res, stopped at i=11\n",
      "r/DissonautUniverse\n",
      "not found r/DissonautUniverse, reason banned\n",
      "get subreddit returned dig_deeper false, stopped at i=1\n",
      "r/gabagoodness\n",
      "empty res, stopped at i=11\n",
      "r/microdosing\n",
      "empty res, stopped at i=11\n",
      "r/noids\n",
      "empty res, stopped at i=11\n",
      "r/opiates\n",
      "empty res, stopped at i=11\n",
      "r/Opioid_RCs\n",
      "empty res, stopped at i=11\n",
      "r/PsychedelicMessages\n",
      "empty res, stopped at i=4\n",
      "r/Psychedelics\n",
      "empty res, stopped at i=11\n",
      "r/PsychedSubstance\n",
      "empty res, stopped at i=11\n",
      "r/Psychonaut\n",
      "empty res, stopped at i=11\n",
      "r/RationalPsychonaut\n",
      "empty res, stopped at i=11\n",
      "r/Stims\n",
      "empty res, stopped at i=11\n",
      "r/treedibles\n",
      "empty res, stopped at i=11\n",
      "r/trees\n",
      "empty res, stopped at i=11\n",
      "r/tryptonaut\n",
      "private subreddit r/tryptonaut\n",
      "get subreddit returned dig_deeper false, stopped at i=1\n",
      "r/AnAnswerToHeal\n",
      "empty res, stopped at i=8\n",
      "r/AskDrugNerds\n",
      "empty res, stopped at i=11\n",
      "r/askdrugs\n",
      "empty res, stopped at i=11\n",
      "r/aves\n",
      "empty res, stopped at i=11\n",
      "r/bestoferowid\n",
      "empty res, stopped at i=3\n",
      "r/CurrentlyTripping\n",
      "empty res, stopped at i=11\n",
      "r/druganalytics\n",
      "empty res, stopped at i=2\n",
      "r/druggardening\n",
      "empty res, stopped at i=11\n",
      "r/DrugNerds\n",
      "empty res, stopped at i=11\n",
      "r/Drugs\n",
      "empty res, stopped at i=11\n",
      "r/DrugShowerThoughts\n",
      "empty res, stopped at i=4\n",
      "r/DrugsOver30\n",
      "empty res, stopped at i=11\n",
      "r/erowid\n",
      "empty res, stopped at i=3\n",
      "r/ObscureDrugs\n",
      "empty res, stopped at i=11\n",
      "r/ReagentTesting\n",
      "empty res, stopped at i=11\n",
      "r/researchchemicals\n",
      "empty res, stopped at i=11\n",
      "r/samelevel\n",
      " exception while checking error <Response [500]>\n",
      "<Response [500]>\n",
      "empty res, stopped at i=4\n",
      "r/TheDrugClassroom\n",
      "empty res, stopped at i=4\n",
      "r/TheeHive\n",
      "empty res, stopped at i=11\n",
      "r/tripreports\n",
      "empty res, stopped at i=11\n",
      "r/TripSit\n",
      "empty res, stopped at i=11\n",
      "r/TripTales\n",
      "empty res, stopped at i=3\n",
      "time to scrap 15814.808827877045\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prev_data_from_broken_loop=data.copy()\n",
    "    prev_data_from_broken_loop.set_index(['subreddit', 'id'], inplace=True)\n",
    "    print(f'prevous data frame already exists with {len(data)} entries, continue the broken loop')\n",
    "    continue_broken_loop = True\n",
    "    \n",
    "except:\n",
    "    data = pd.DataFrame()\n",
    "    print('data initialised as empty')\n",
    "    continue_broken_loop = False\n",
    "\n",
    "start = time.time()\n",
    "for sub_reddit in sub_reddits:\n",
    "    \n",
    "    params = {'limit': 100}\n",
    "    print(sub_reddit)\n",
    "    if load_from_begining==False:# set the last known data entry\n",
    "        try:\n",
    "            params['before'] = prev_data.loc[sub_reddit[2:]].iloc[0]['name']\n",
    "            i=0\n",
    "        except:\n",
    "            print(f'No {sub_reddit} in previous data set, continue to the next one')\n",
    "            continue\n",
    "    elif continue_broken_loop: # loop was broken\n",
    "        try:\n",
    "            params['after'] = prev_data_from_broken_loop.loc[prev_data_from_broken_loop['body'].isna()].loc[sub_reddit[2:]].iloc[-1]['name']\n",
    "            i = len(prev_data_from_broken_loop.loc[prev_data_from_broken_loop['body'].isna()].loc[sub_reddit[2:]])/100\n",
    "            i = math.floor(i)\n",
    "            print(i)\n",
    "        except:\n",
    "            print(f'No {sub_reddit} in prev data set, loading from the begining')\n",
    "            params = {'limit': 100}\n",
    "            i=0\n",
    "    else:\n",
    "        i=0\n",
    "\n",
    "    year_of_comments = datetime.now().year\n",
    "    dig_deeper = True\n",
    "    while dig_deeper:\n",
    "        i += 1\n",
    "        dig_deeper = i<=100 | year_of_comments>2010\n",
    "\n",
    "        res, dig_deeper = get_subreddit(sub_reddit, headers, params, dig_deeper)\n",
    "        if dig_deeper==False:\n",
    "            print(f'get subreddit returned dig_deeper false, stopped at i={i}')\n",
    "            continue\n",
    "        \n",
    "\n",
    "        new_df = df_from_response(res) #create new data frame\n",
    "        if len(new_df)==0:\n",
    "            dig_deeper=False\n",
    "            print(f'empty res, stopped at i={i}')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if load_from_begining:\n",
    "            # take the final row (oldest entry) not the comments\n",
    "            if 'body' in new_df:\n",
    "                row = new_df.loc[new_df['body'].isna()].iloc[-1] \n",
    "            else:\n",
    "                row = new_df.iloc[-1]             \n",
    "            params['after'] = row['name']\n",
    "            a = pd.to_datetime(row['created_utc'])\n",
    "            year_of_comments = a.year\n",
    "\n",
    "        else:\n",
    "            # take the first row (newest entry)\n",
    "            if 'body' in new_df:\n",
    "                row = new_df.loc[new_df['body'].isna()].iloc[0]              \n",
    "            else:\n",
    "                row = new_df.iloc[0]\n",
    "            # add/update fullname in params\n",
    "            params['before'] = row['name']\n",
    "          \n",
    "\n",
    "        # append new_df to data\n",
    "        data = pd.concat([data, new_df], ignore_index=True)\n",
    "end = time.time()\n",
    "print('time to scrap',end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce21b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>view_count</th>\n",
       "      <th>score</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>kind</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1P_LSD</th>\n",
       "      <th>j56fqvn</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j56fqvn</td>\n",
       "      <td>Miley-Jay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-20T19:53:27Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>Unless it is a lab or vendor for a lab anythin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j55zgsj</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j55zgsj</td>\n",
       "      <td>nowayfly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-20T18:12:30Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>Sure Fent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j55xsbx</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j55xsbx</td>\n",
       "      <td>Environmental-Fan196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-20T18:01:58Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>Left are LL the others idk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j55vtlh</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j55vtlh</td>\n",
       "      <td>Potato-Interesting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-20T17:49:53Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>The one on the left looks like the ones I have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j55q3nc</th>\n",
       "      <td>r/1P_LSD</td>\n",
       "      <td>t1_j55q3nc</td>\n",
       "      <td>Realistic_Froyo_952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-20T17:13:49Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>I have got white paper and yellowish paper all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">tripreports</th>\n",
       "      <th>otqz2r</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t3_otqz2r</td>\n",
       "      <td>anusmold</td>\n",
       "      <td>Forgot I took DMT during the trip itself</td>\n",
       "      <td>So I believe I had my first breakthrough. I di...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-29T07:06:35Z</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h6x7m57</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t1_h6x7m57</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-29T07:06:35Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>\\nRemember to be civil!\\n\\nDon't be afraid to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h6px1cc</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t1_h6px1cc</td>\n",
       "      <td>Psych_Art</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-27T17:55:58Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>Just want to clarify that 4-HO-MET is not a pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>osohl1</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t3_osohl1</td>\n",
       "      <td>OtherwiseWePanic</td>\n",
       "      <td>1P-LSD + 4-HO-MET</td>\n",
       "      <td>1P-LSD + 4-HO-MET\\n\\n**(300ug 1P-LSD with 50mg...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-27T16:59:31Z</td>\n",
       "      <td>t3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h6ppewh</th>\n",
       "      <td>r/tripreports</td>\n",
       "      <td>t1_h6ppewh</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-27T16:59:31Z</td>\n",
       "      <td>t1</td>\n",
       "      <td>\\nRemember to be civil!\\n\\nDon't be afraid to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310422 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subreddit_name_prefixed        name                author  \\\n",
       "subreddit   id                                                                  \n",
       "1P_LSD      j56fqvn                r/1P_LSD  t1_j56fqvn             Miley-Jay   \n",
       "            j55zgsj                r/1P_LSD  t1_j55zgsj              nowayfly   \n",
       "            j55xsbx                r/1P_LSD  t1_j55xsbx  Environmental-Fan196   \n",
       "            j55vtlh                r/1P_LSD  t1_j55vtlh    Potato-Interesting   \n",
       "            j55q3nc                r/1P_LSD  t1_j55q3nc   Realistic_Froyo_952   \n",
       "...                                     ...         ...                   ...   \n",
       "tripreports otqz2r            r/tripreports   t3_otqz2r              anusmold   \n",
       "            h6x7m57           r/tripreports  t1_h6x7m57         AutoModerator   \n",
       "            h6px1cc           r/tripreports  t1_h6px1cc             Psych_Art   \n",
       "            osohl1            r/tripreports   t3_osohl1      OtherwiseWePanic   \n",
       "            h6ppewh           r/tripreports  t1_h6ppewh         AutoModerator   \n",
       "\n",
       "                                                        title  \\\n",
       "subreddit   id                                                  \n",
       "1P_LSD      j56fqvn                                       NaN   \n",
       "            j55zgsj                                       NaN   \n",
       "            j55xsbx                                       NaN   \n",
       "            j55vtlh                                       NaN   \n",
       "            j55q3nc                                       NaN   \n",
       "...                                                       ...   \n",
       "tripreports otqz2r   Forgot I took DMT during the trip itself   \n",
       "            h6x7m57                                       NaN   \n",
       "            h6px1cc                                       NaN   \n",
       "            osohl1                          1P-LSD + 4-HO-MET   \n",
       "            h6ppewh                                       NaN   \n",
       "\n",
       "                                                              selftext  \\\n",
       "subreddit   id                                                           \n",
       "1P_LSD      j56fqvn                                                NaN   \n",
       "            j55zgsj                                                NaN   \n",
       "            j55xsbx                                                NaN   \n",
       "            j55vtlh                                                NaN   \n",
       "            j55q3nc                                                NaN   \n",
       "...                                                                ...   \n",
       "tripreports otqz2r   So I believe I had my first breakthrough. I di...   \n",
       "            h6x7m57                                                NaN   \n",
       "            h6px1cc                                                NaN   \n",
       "            osohl1   1P-LSD + 4-HO-MET\\n\\n**(300ug 1P-LSD with 50mg...   \n",
       "            h6ppewh                                                NaN   \n",
       "\n",
       "                    num_comments upvote_ratio ups downs view_count score  \\\n",
       "subreddit   id                                                             \n",
       "1P_LSD      j56fqvn          NaN          NaN   1     0        NaN     1   \n",
       "            j55zgsj          NaN          NaN  -3     0        NaN    -3   \n",
       "            j55xsbx          NaN          NaN   0     0        NaN     0   \n",
       "            j55vtlh          NaN          NaN   3     0        NaN     3   \n",
       "            j55q3nc          NaN          NaN   2     0        NaN     2   \n",
       "...                          ...          ...  ..   ...        ...   ...   \n",
       "tripreports otqz2r             3          1.0   1     0       None     1   \n",
       "            h6x7m57          NaN          NaN   1     0        NaN     1   \n",
       "            h6px1cc          NaN          NaN   4     0        NaN     4   \n",
       "            osohl1             9          1.0  14     0       None    14   \n",
       "            h6ppewh          NaN          NaN   1     0        NaN     1   \n",
       "\n",
       "                    link_flair_css_class           created_utc kind  \\\n",
       "subreddit   id                                                        \n",
       "1P_LSD      j56fqvn                  NaN  2023-01-20T19:53:27Z   t1   \n",
       "            j55zgsj                  NaN  2023-01-20T18:12:30Z   t1   \n",
       "            j55xsbx                  NaN  2023-01-20T18:01:58Z   t1   \n",
       "            j55vtlh                  NaN  2023-01-20T17:49:53Z   t1   \n",
       "            j55q3nc                  NaN  2023-01-20T17:13:49Z   t1   \n",
       "...                                  ...                   ...  ...   \n",
       "tripreports otqz2r                        2021-07-29T07:06:35Z   t3   \n",
       "            h6x7m57                  NaN  2021-07-29T07:06:35Z   t1   \n",
       "            h6px1cc                  NaN  2021-07-27T17:55:58Z   t1   \n",
       "            osohl1                        2021-07-27T16:59:31Z   t3   \n",
       "            h6ppewh                  NaN  2021-07-27T16:59:31Z   t1   \n",
       "\n",
       "                                                                  body  \n",
       "subreddit   id                                                          \n",
       "1P_LSD      j56fqvn  Unless it is a lab or vendor for a lab anythin...  \n",
       "            j55zgsj                                          Sure Fent  \n",
       "            j55xsbx                         Left are LL the others idk  \n",
       "            j55vtlh  The one on the left looks like the ones I have...  \n",
       "            j55q3nc  I have got white paper and yellowish paper all...  \n",
       "...                                                                ...  \n",
       "tripreports otqz2r                                                 NaN  \n",
       "            h6x7m57  \\nRemember to be civil!\\n\\nDon't be afraid to ...  \n",
       "            h6px1cc  Just want to clarify that 4-HO-MET is not a pr...  \n",
       "            osohl1                                                 NaN  \n",
       "            h6ppewh  \\nRemember to be civil!\\n\\nDon't be afraid to ...  \n",
       "\n",
       "[310422 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if load_from_begining:\n",
    "    data_combined_with_delta = data\n",
    "else:\n",
    "    data_combined_with_delta = pd.concat([data, prev_data.reset_index()], ignore_index=True)\n",
    "    \n",
    "data_combined_with_delta.sort_values(by=['subreddit', 'created_utc'], ascending=(True,False), inplace=True)\n",
    "data_combined_with_delta.set_index(['subreddit', 'id'], inplace=True)\n",
    "\n",
    "display(data_combined_with_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27fffb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: cannot stat 'objs.pkl': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_combined_with_delta.to_csv('reddit.csv')  \n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.system('cp objs.pkl objs.pkl.bkp')\n",
    "except:\n",
    "    print('no previous version found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20fbdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "data_combined_with_delta['year_month'] = \\\n",
    "     pd.to_datetime(pd.to_datetime(data_combined_with_delta['created_utc']).dt.strftime('%Y-%m'))\n",
    "with open('objs.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(data_combined_with_delta, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
