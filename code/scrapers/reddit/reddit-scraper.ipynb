{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import date\n",
    "import pwlf\n",
    "import datetime\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time, math\n",
    "\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('CllfFuOvEOBS09TAzabT9Q', 'UmOCgu0uvbPzHn6bJgPwZ5Yf6Rmx7w')\n",
    "\n",
    "with open('pw.txt','r') as f:\n",
    "    pw = f.read().rstrip('\\n')\n",
    "# here we pass our login method (password), username, and password\n",
    "login = {'grant_type': 'password',\n",
    "        'username': 'dspd1',\n",
    "        'password': pw}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db524b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers(login):\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=login, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers['Authorization'] = f'bearer {TOKEN}'\n",
    "\n",
    "    # while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "    requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b06677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_reddits=['r/1P_LSD', 'r/2cb', 'r/4acodmt', 'r/adderall', 'r/afinil', 'r/ambien', 'r/AMT', 'r/Ayahuasca', \n",
    "'r/cocaine', 'r/cripplingalcoholism', 'r/DMT', 'r/DPH', 'r/dxm', 'r/fentanyl', 'r/ketamine', 'r/kratom', \n",
    "'r/LSA', 'r/LSD', 'r/MDMA', 'r/MemantineHCl', 'r/mescaline', 'r/meth', 'r/PCP', \n",
    "'r/phenibut', 'r/PsilocybinMushrooms', 'r/Salvia', 'r/shroomers', 'r/shrooms', \n",
    "'r/benzodiazepines', 'r/dissociatives', 'r/DissonautUniverse', 'r/gabagoodness', \n",
    "'r/microdosing', 'r/noids', 'r/opiates', 'r/Opioid_RCs', 'r/PsychedelicMessages', \n",
    "'r/Psychedelics', 'r/PsychedSubstance', 'r/Psychonaut', 'r/RationalPsychonaut', 'r/Stims', \n",
    "'r/treedibles', 'r/trees', 'r/tryptonaut', 'r/AnAnswerToHeal', 'r/AskDrugNerds', 'r/askdrugs', 'r/aves', \n",
    "'r/bestoferowid', 'r/CurrentlyTripping', 'r/druganalytics', 'r/druggardening', 'r/DrugNerds', 'r/Drugs', \n",
    "'r/DrugShowerThoughts', 'r/DrugsOver30', 'r/erowid', 'r/ObscureDrugs', 'r/ReagentTesting', 'r/researchchemicals', \n",
    "'r/samelevel', 'r/TheDrugClassroom', 'r/TheeHive', 'r/tripreports', 'r/TripSit', 'r/TripTales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02878996",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = get_headers(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f928bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit(subreddit, headers, params, dig_deeper):\n",
    "    res = requests.get(f'https://oauth.reddit.com/{sub_reddit}/new',\n",
    "                              headers=headers,\n",
    "                              params=params)\n",
    "    if 'error' in res.json():\n",
    "        if res.json()['error']==403:\n",
    "            print(f'private subreddit {sub_reddit}')\n",
    "        elif res.json()['error']==404:\n",
    "            reason = res.json()['reason']\n",
    "            print(f'not found {sub_reddit}, reason {reason}')\n",
    "        else:\n",
    "            print(res.json())\n",
    "        \n",
    "        dig_deeper = False\n",
    "    \n",
    "    return res, dig_deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7050d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_from_post(subreddit, post_id):\n",
    "    df_from_comments = pd.DataFrame()\n",
    "\n",
    "    res_comments = requests.get(f'https://oauth.reddit.com/{subreddit}/comments/{post_id}', \n",
    "               params={'depth':3,'sort':'old', 'limit':30}, headers=headers)\n",
    "    try:\n",
    "        if 'error' in res_comments.json():\n",
    "            print(f'error in comments {res_comments.json()}')\n",
    "    except:\n",
    "        print(f' exception while checking error {res_comments}')\n",
    "        \n",
    "    try:\n",
    "        if len(res_comments.json())>1:\n",
    "            df_from_comments = df_comments(res_comments)\n",
    "    except:\n",
    "        print(res_comments)\n",
    "\n",
    "    return df_from_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response(res):\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        df = pd.concat([df, pd.Series({\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'subreddit_name_prefixed': post['data']['subreddit_name_prefixed'],\n",
    "            'name': post['data']['name'],\n",
    "            'author': post['data']['author'],\n",
    "            'title': post['data']['title'],\n",
    "            'selftext': post['data']['selftext'],\n",
    "            'num_comments':post['data']['num_comments'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'view_count': post['data']['view_count'],\n",
    "            'score': post['data']['score'],\n",
    "            'link_flair_css_class': post['data']['link_flair_css_class'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'id': post['data']['id'],\n",
    "            'kind': post['kind']}).to_frame().T], ignore_index=True)\n",
    "        subreddit = post['data']['subreddit_name_prefixed']\n",
    "        id_ = post['data']['id']\n",
    "#         print(subreddit)\n",
    "#         print(id_)\n",
    "        df_from_comments = get_comments_from_post(subreddit=subreddit, post_id=id_)\n",
    "        df = pd.concat([df, df_from_comments], ignore_index=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_comments(res):\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()[1]['data']['children']:\n",
    "        if post['kind']=='t1':\n",
    "            df = pd.concat([df, pd.Series({\n",
    "                'subreddit': post['data']['subreddit'],\n",
    "                'subreddit_name_prefixed': post['data']['subreddit_name_prefixed'],\n",
    "                'name': post['data']['name'],\n",
    "                'author': post['data']['author'],\n",
    "                'body': post['data']['body'],\n",
    "                'ups': post['data']['ups'],\n",
    "                'downs': post['data']['downs'],\n",
    "                'score': post['data']['score'],\n",
    "                'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "                'id': post['data']['id'],\n",
    "                'kind': post['kind']}).to_frame().T], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# 'approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', \n",
    "# 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', \n",
    "# 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', \n",
    "# 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', \n",
    "# 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', \n",
    "# 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', \n",
    "# 'is_created_from_ads_ui', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', \n",
    "# 'author_flair_richtext', 'gildings', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type',\n",
    "# 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', \n",
    "# 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', \n",
    "# 'over_18', 'all_awardings', 'awarders', 'media_only', 'can_gild', 'spoiler', 'locked', 'author_flair_text', \n",
    "# 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'author_is_blocked', \n",
    "# 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', \n",
    "# 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', \n",
    "# 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', \n",
    "# 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fa4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # prev_data = pd.read_csv('reddit.csv')\n",
    "    # prev_data.set_index(['subreddit', 'id'], inplace=True)\n",
    "    # display(prev_data)\n",
    "    # Getting back the objects:\n",
    "    with open('../../../final_dashboard_D1/objs.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        prev_data = pickle.load(f)\n",
    "        \n",
    "    load_from_begining = False\n",
    "    print(\"Loaded previous data base - loading delta\")\n",
    "\n",
    "except:\n",
    "    print(f'No previous database, loading from the beginning')\n",
    "    prev_data = pd.DataFrame()\n",
    "    load_from_begining = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c4dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    prev_data_from_broken_loop=data.copy()\n",
    "    prev_data_from_broken_loop.set_index(['subreddit', 'id'], inplace=True)\n",
    "    print(f'prevous data frame already exists with {len(data)} entries, continue the broken loop')\n",
    "    continue_broken_loop = True\n",
    "    \n",
    "except:\n",
    "    data = pd.DataFrame()\n",
    "    print('data initialised as empty')\n",
    "    continue_broken_loop = False\n",
    "\n",
    "start = time.time()\n",
    "for sub_reddit in sub_reddits:\n",
    "    \n",
    "    params = {'limit': 100}\n",
    "    print(sub_reddit)\n",
    "    if load_from_begining==False:# set the last known data entry\n",
    "        try:\n",
    "            params['before'] = prev_data.loc[sub_reddit[2:]].iloc[0]['name']\n",
    "            i=0\n",
    "        except:\n",
    "            print(f'No {sub_reddit} in previous data set, continue to the next one')\n",
    "            continue\n",
    "    elif continue_broken_loop: # loop was broken\n",
    "        try:\n",
    "            params['after'] = prev_data_from_broken_loop.loc[prev_data_from_broken_loop['body'].isna()].loc[sub_reddit[2:]].iloc[-1]['name']\n",
    "            i = len(prev_data_from_broken_loop.loc[prev_data_from_broken_loop['body'].isna()].loc[sub_reddit[2:]])/100\n",
    "            i = math.floor(i)\n",
    "            print(i)\n",
    "        except:\n",
    "            print(f'No {sub_reddit} in prev data set, loading from the begining')\n",
    "            params = {'limit': 100}\n",
    "            i=0\n",
    "    else:\n",
    "        i=0\n",
    "\n",
    "    year_of_comments = datetime.now().year\n",
    "    dig_deeper = True\n",
    "    while dig_deeper:\n",
    "        i += 1\n",
    "        dig_deeper = i<=100 | year_of_comments>2010\n",
    "\n",
    "        res, dig_deeper = get_subreddit(sub_reddit, headers, params, dig_deeper)\n",
    "        if dig_deeper==False:\n",
    "            print(f'get subreddit returned dig_deeper false, stopped at i={i}')\n",
    "            continue\n",
    "        \n",
    "\n",
    "        new_df = df_from_response(res) #create new data frame\n",
    "        if len(new_df)==0:\n",
    "            dig_deeper=False\n",
    "            print(f'empty res, stopped at i={i}')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if load_from_begining:\n",
    "            # take the final row (oldest entry) not the comments\n",
    "            if 'body' in new_df:\n",
    "                row = new_df.loc[new_df['body'].isna()].iloc[-1] \n",
    "            else:\n",
    "                row = new_df.iloc[-1]             \n",
    "            params['after'] = row['name']\n",
    "            a = pd.to_datetime(row['created_utc'])\n",
    "            year_of_comments = a.year\n",
    "\n",
    "        else:\n",
    "            # take the first row (newest entry)\n",
    "            if 'body' in new_df:\n",
    "                row = new_df.loc[new_df['body'].isna()].iloc[0]              \n",
    "            else:\n",
    "                row = new_df.iloc[0]\n",
    "            # add/update fullname in params\n",
    "            params['before'] = row['name']\n",
    "          \n",
    "\n",
    "        # append new_df to data\n",
    "        data = pd.concat([data, new_df], ignore_index=True)\n",
    "end = time.time()\n",
    "print('time to scrap',end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce21b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_begining:\n",
    "    data_combined_with_delta = data\n",
    "else:\n",
    "    data_combined_with_delta = pd.concat([data, prev_data.reset_index()], ignore_index=True)\n",
    "    \n",
    "data_combined_with_delta.sort_values(by=['subreddit', 'created_utc'], ascending=(True,False), inplace=True)\n",
    "data_combined_with_delta.set_index(['subreddit', 'id'], inplace=True)\n",
    "\n",
    "display(data_combined_with_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fffb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_combined_with_delta.to_csv('reddit.csv')  \n",
    "import os\n",
    "os.system\n",
    "try:\n",
    "    os.system('cp ../../../final_dashboard_D1/objs.pkl ../../../final_dashboard_D1/objs.pkl.bkp')\n",
    "except:\n",
    "    print('no previous version found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "data_combined_with_delta['year_month'] = \\\n",
    "     pd.to_datetime(pd.to_datetime(data_combined_with_delta['created_utc']).dt.strftime('%Y-%m'))\n",
    "with open('../../../final_dashboard_D1/objs.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(data_combined_with_delta, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
